{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd56fff-49b7-4af5-8e20-c855825e780c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea40afaa-a9ba-4c31-86fe-ab73629c86aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e374824-0c7d-44dd-b661-a0cce0c96d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .appName(\"Python Spark SQL basic example\") \\\n",
    " .config(\"spark.some.config.option\", \"some-value\") \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c11a3d9-ed7b-40fc-9f41-e46749dd114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|1,A, 2023-01-01,I...|\n",
      "|2,A, 2022-01-01,I...|\n",
      "|2,A, 2023-01-07,I...|\n",
      "|3,A, 2023-01-10,I...|\n",
      "|3,A, 2022-01-11,I...|\n",
      "|3,A, 2023-01-11,I...|\n",
      "|2,B, 2022-02-01,I...|\n",
      "|2,B, 2023-01-02,I...|\n",
      "|1,B, 2023-01-04,I...|\n",
      "|1,B, 2023-02-11,I...|\n",
      "|3,B, 2023-01-16,I...|\n",
      "|3,B, 2022-02-01,I...|\n",
      "|3,C, 2023-01-01,I...|\n",
      "|1,C, 2023-01-01,U...|\n",
      "|6,C, 2022-01-07,U...|\n",
      "|3,D, 2023-02-16,U...|\n",
      "|5,D, 2022-02-01,U...|\n",
      "|3,E, 2023-02-01,U...|\n",
      "|4,E, 2023-02-01,U...|\n",
      "|4,E, 2023-02-07,U...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "  \n",
    "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate() \n",
    "  \n",
    "Sale_df = spark.read.text(r\"B:\\mdata\\Download\\sales.csv.txt\") \n",
    "  \n",
    "Sale_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb83ad-0272-4a44-aefa-ec008755070a",
   "metadata": {},
   "source": [
    "**Sale DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "529ea950-cd5c-4805-a450-40b8ad99f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType ,DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39e663dd-3391-40bf-9a1f-1b47b3c3bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product_id: integer (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- source_order: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session using getOrCreate() function \n",
    "spark_session = SparkSession.builder.getOrCreate() \n",
    "  \n",
    "# Define the structure for the data frame \n",
    "schema = StructType([ \n",
    "    StructField(\"Product_id\",IntegerType(),True),\n",
    "    StructField(\"customer_id\",StringType(),True),\n",
    "    StructField(\"order_date\",DateType(),True),\n",
    "    StructField(\"location\",StringType(),True),\n",
    "    StructField(\"source_order\",StringType(),True),\n",
    "]) \n",
    "  \n",
    "# Applying custom schema to data frame \n",
    "Sale_df = spark_session.read.format( \n",
    "    \"csv\").schema(schema).option( \n",
    "    \"header\", False).load(r\"B:\\mdata\\Download\\sales.csv.txt\") \n",
    "  \n",
    "# Display the updated schema \n",
    "Sale_df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecee8750-4fcd-4d60-8c68-fbaf881c022e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Product_id: int, customer_id: int, order_date: int, location: int, source_order: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Sale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc05290e-e014-44d1-997d-1946357c1d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+--------+------------+\n",
      "|Product_id|customer_id| order_date|location|source_order|\n",
      "+----------+-----------+-----------+--------+------------+\n",
      "|         1|          A| 2023-01-01|   India|      Swiggy|\n",
      "|         2|          A| 2022-01-01|   India|      Swiggy|\n",
      "|         2|          A| 2023-01-07|   India|      Swiggy|\n",
      "|         3|          A| 2023-01-10|   India|  Restaurant|\n",
      "|         3|          A| 2022-01-11|   India|      Swiggy|\n",
      "|         3|          A| 2023-01-11|   India|  Restaurant|\n",
      "|         2|          B| 2022-02-01|   India|      Swiggy|\n",
      "|         2|          B| 2023-01-02|   India|      Swiggy|\n",
      "|         1|          B| 2023-01-04|   India|  Restaurant|\n",
      "|         1|          B| 2023-02-11|   India|      Swiggy|\n",
      "|         3|          B| 2023-01-16|   India|      zomato|\n",
      "|         3|          B| 2022-02-01|   India|      zomato|\n",
      "|         3|          C| 2023-01-01|   India|      zomato|\n",
      "|         1|          C| 2023-01-01|      UK|      Swiggy|\n",
      "|         6|          C| 2022-01-07|      UK|      zomato|\n",
      "|         3|          D| 2023-02-16|      UK|  Restaurant|\n",
      "|         5|          D| 2022-02-01|      UK|      zomato|\n",
      "|         3|          E| 2023-02-01|      UK|  Restaurant|\n",
      "|         4|          E| 2023-02-01|      UK|      Swiggy|\n",
      "|         4|          E| 2023-02-07|      UK|  Restaurant|\n",
      "+----------+-----------+-----------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sale_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f54005f-6810-4c0f-9002-83873a55919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Product_id: int, customer_id: string, order_date: date, location: string, source_order: string, order_year: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Deriving data from year,Month,Quater\n",
    "\n",
    "from pyspark.sql. functions import month, year, quarter\n",
    "Sale_df=Sale_df.withColumn (\"order_year\", year (Sale_df.order_date))\n",
    "display(Sale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5f5680f-966b-42c9-b7ac-2ecdff6ed505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+--------+------------+----------+\n",
      "|Product_id|customer_id|order_date|location|source_order|order_year|\n",
      "+----------+-----------+----------+--------+------------+----------+\n",
      "|         1|          A|2023-01-01|   India|      Swiggy|      2023|\n",
      "|         2|          A|2022-01-01|   India|      Swiggy|      2022|\n",
      "|         2|          A|2023-01-07|   India|      Swiggy|      2023|\n",
      "|         3|          A|2023-01-10|   India|  Restaurant|      2023|\n",
      "|         3|          A|2022-01-11|   India|      Swiggy|      2022|\n",
      "|         3|          A|2023-01-11|   India|  Restaurant|      2023|\n",
      "|         2|          B|2022-02-01|   India|      Swiggy|      2022|\n",
      "|         2|          B|2023-01-02|   India|      Swiggy|      2023|\n",
      "|         1|          B|2023-01-04|   India|  Restaurant|      2023|\n",
      "|         1|          B|2023-02-11|   India|      Swiggy|      2023|\n",
      "|         3|          B|2023-01-16|   India|      zomato|      2023|\n",
      "|         3|          B|2022-02-01|   India|      zomato|      2022|\n",
      "|         3|          C|2023-01-01|   India|      zomato|      2023|\n",
      "|         1|          C|2023-01-01|      UK|      Swiggy|      2023|\n",
      "|         6|          C|2022-01-07|      UK|      zomato|      2022|\n",
      "|         3|          D|2023-02-16|      UK|  Restaurant|      2023|\n",
      "|         5|          D|2022-02-01|      UK|      zomato|      2022|\n",
      "|         3|          E|2023-02-01|      UK|  Restaurant|      2023|\n",
      "|         4|          E|2023-02-01|      UK|      Swiggy|      2023|\n",
      "|         4|          E|2023-02-07|      UK|  Restaurant|      2023|\n",
      "+----------+-----------+----------+--------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sale_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9517971-4cff-4c3e-8639-762d0c1fdbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Product_id: int, customer_id: string, order_date: date, location: string, source_order: string, order_year: int, order_Month: int, order_Quarter: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
      "|Product_id|customer_id|order_date|location|source_order|order_year|order_Month|order_Quarter|\n",
      "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
      "|         1|          A|2023-01-01|   India|      Swiggy|      2023|          1|            1|\n",
      "|         2|          A|2022-01-01|   India|      Swiggy|      2022|          1|            1|\n",
      "|         2|          A|2023-01-07|   India|      Swiggy|      2023|          1|            1|\n",
      "|         3|          A|2023-01-10|   India|  Restaurant|      2023|          1|            1|\n",
      "|         3|          A|2022-01-11|   India|      Swiggy|      2022|          1|            1|\n",
      "|         3|          A|2023-01-11|   India|  Restaurant|      2023|          1|            1|\n",
      "|         2|          B|2022-02-01|   India|      Swiggy|      2022|          2|            1|\n",
      "|         2|          B|2023-01-02|   India|      Swiggy|      2023|          1|            1|\n",
      "|         1|          B|2023-01-04|   India|  Restaurant|      2023|          1|            1|\n",
      "|         1|          B|2023-02-11|   India|      Swiggy|      2023|          2|            1|\n",
      "|         3|          B|2023-01-16|   India|      zomato|      2023|          1|            1|\n",
      "|         3|          B|2022-02-01|   India|      zomato|      2022|          2|            1|\n",
      "|         3|          C|2023-01-01|   India|      zomato|      2023|          1|            1|\n",
      "|         1|          C|2023-01-01|      UK|      Swiggy|      2023|          1|            1|\n",
      "|         6|          C|2022-01-07|      UK|      zomato|      2022|          1|            1|\n",
      "|         3|          D|2023-02-16|      UK|  Restaurant|      2023|          2|            1|\n",
      "|         5|          D|2022-02-01|      UK|      zomato|      2022|          2|            1|\n",
      "|         3|          E|2023-02-01|      UK|  Restaurant|      2023|          2|            1|\n",
      "|         4|          E|2023-02-01|      UK|      Swiggy|      2023|          2|            1|\n",
      "|         4|          E|2023-02-07|      UK|  Restaurant|      2023|          2|            1|\n",
      "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sale_df=Sale_df.withColumn (\"order_Month\", month (Sale_df.order_date))\n",
    "Sale_df=Sale_df.withColumn (\"order_Quarter\", quarter (Sale_df.order_date))\n",
    "display(Sale_df)\n",
    "Sale_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd2daa-241c-423d-862b-2b12cce5dcd9",
   "metadata": {},
   "source": [
    "**Menu DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67da8379-7082-44dd-bdaf-e9f75c70318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product_id: integer (nullable = true)\n",
      " |-- Product_Name: string (nullable = true)\n",
      " |-- Prize: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType ,DateType\n",
    "\n",
    "# Create a spark session using getOrCreate() function \n",
    "spark_session = SparkSession.builder.getOrCreate() \n",
    "  \n",
    "# Define the structure for the data frame \n",
    "schema = StructType([ \n",
    "    StructField(\"Product_id\",IntegerType(),True),\n",
    "    StructField(\"Product_Name\",StringType(),True),\n",
    "    StructField(\"Prize\",StringType(),True),\n",
    "]) \n",
    "  \n",
    "# Applying custom schema to data frame \n",
    "menu_df = spark_session.read.format( \n",
    "    \"csv\").schema(schema).option( \n",
    "    \"header\", False).load(r\"B:\\mdata\\Download\\menu.csv.txt\") \n",
    "  \n",
    "# Display the updated schema \n",
    "menu_df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ac92d58-11af-4563-a27f-d8b93068c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|Product_id|Product_Name|Prize|\n",
      "+----------+------------+-----+\n",
      "|         1|       PIZZA|  100|\n",
      "|         2|     Chowmin|  150|\n",
      "|         3|    sandwich|  120|\n",
      "|         4|        Dosa|  110|\n",
      "|         5|     Biryani|   80|\n",
      "|         6|       Pasta|  180|\n",
      "+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "menu_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd28daa-4a05-43d2-8136-3738fd1c736e",
   "metadata": {},
   "source": [
    "**Total Amount Spend By Each Customer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cfd0ca51-ca35-4739-91b4-8a47755b1712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|sum(Prize)|\n",
      "+-----------+----------+\n",
      "|          A|    4260.0|\n",
      "|          B|    4440.0|\n",
      "|          C|    2400.0|\n",
      "|          D|    1200.0|\n",
      "|          E|    2040.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Total_amount_spent = (Sale_df.join(menu_df, 'Product_id').groupBy( 'customer_id').agg({'Prize': 'sum' }).orderBy('customer_id'))\n",
    "Total_amount_spent.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9e69e-2090-450e-8c18-0bfa59724ece",
   "metadata": {},
   "source": [
    "**Total Amount Spend By Each Food Categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af326314-9434-4193-8d74-64ee62e0bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|Product_Name|sum(Prize)|\n",
      "+------------+----------+\n",
      "|     Biryani|     480.0|\n",
      "|     Chowmin|    3600.0|\n",
      "|        Dosa|    1320.0|\n",
      "|       PIZZA|    2100.0|\n",
      "|       Pasta|    1080.0|\n",
      "|    sandwich|    5760.0|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Total_Spent_InFood = (Sale_df.join(menu_df, 'Product_id').groupBy( 'Product_Name').agg({'Prize': 'Sum' }).orderBy('Product_Name'))\n",
    "Total_Spent_InFood.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57705f8-c7bb-41d4-97e1-0ace165c9039",
   "metadata": {},
   "source": [
    "**Total Amount Of Sale Each Month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8ea92bf-d7ee-48ad-ad15-4ed429453e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|order_Month|sum(Prize)|\n",
      "+-----------+----------+\n",
      "|          1|    2960.0|\n",
      "|          2|    2730.0|\n",
      "|          3|     910.0|\n",
      "|          5|    2960.0|\n",
      "|          6|    2960.0|\n",
      "|          7|     910.0|\n",
      "|         11|     910.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Total_Spent_InMonth = (Sale_df.join(menu_df, 'Product_id').groupBy( 'order_Month').agg({'Prize': 'Sum' }).orderBy('order_Month'))\n",
    "Total_Spent_InMonth.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881d521-94a8-4336-8771-cc69a0ac73c2",
   "metadata": {},
   "source": [
    "**Yearly Sale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d20fa13-68b6-40c6-b353-991bf54d0c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|order_year|sum(Prize)|\n",
      "+----------+----------+\n",
      "|      2022|    4350.0|\n",
      "|      2023|    9990.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Total_Spent_InYear = (Sale_df.join(menu_df, 'Product_id').groupBy( 'order_year').agg({'Prize': 'Sum' }).orderBy('order_year'))\n",
    "Total_Spent_InYear.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6473c-b4a0-4e8c-af45-fecafefe6f7f",
   "metadata": {},
   "source": [
    "**Quaterly Sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e4c5dbf-a8f9-4665-b1ce-39fa3c2815a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|order_Quarter|sum(Prize)|\n",
      "+-------------+----------+\n",
      "|            1|    6600.0|\n",
      "|            2|    5920.0|\n",
      "|            3|     910.0|\n",
      "|            4|     910.0|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Total_Spent_InQuarter = (Sale_df.join(menu_df, 'Product_id').groupBy( 'order_Quarter').agg({'Prize': 'Sum' }).orderBy('order_Quarter'))\n",
    "Total_Spent_InQuarter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89977c8-1f16-44e7-b1e3-2e66bcd8f211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
